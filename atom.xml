<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>boiling shares reading and technique</title>
  
  
  <link href="https://jinchunqing1.github.io/atom.xml" rel="self"/>
  
  <link href="https://jinchunqing1.github.io/"/>
  <updated>2023-03-15T22:46:28.169Z</updated>
  <id>https://jinchunqing1.github.io/</id>
  
  <author>
    <name>boiling</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>真万字长文，boiling熬夜肝出无监督学习“聚类算法”</title>
    <link href="https://jinchunqing1.github.io/2023/03/09/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>https://jinchunqing1.github.io/2023/03/09/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</id>
    <published>2023-03-09T11:06:20.404Z</published>
    <updated>2023-03-15T22:46:28.169Z</updated>
    
    <content type="html"><![CDATA[<p>聚类算法是一种无监督学习的算法，用于将数据集中的数据分成不同的聚类或组。聚类算法是数据挖掘和机器学习领域中常见的技术之一，具有广泛的应用。 以下是聚类算法的一些知识点：<br>聚类算法的目的是将数据集划分为不同的组，使得组内的数据点相似度高，组间的相似度低。<br>聚类算法可以分为层次聚类和非层次聚类两种。层次聚类可以分为聚合聚类和分裂聚类两种。<br>常用的聚类算法包括K均值聚类、层次聚类、DBSCAN聚类等。其中，K均值聚类是最常见的聚类算法之一，它将数据集分为K个簇，每个簇的中心点被称为质心。<br>聚类算法的评价指标包括轮廓系数、Calinski-Harabasz指数、Davies-Bouldin指数等。<br>聚类算法的应用包括客户分群、文本聚类、图像分割等。<br>聚类算法的优化方法包括选择合适的聚类算法、确定合适的聚类簇数、选择合适的相似度度量等。<br>聚类算法的局限性包括需要事先确定聚类簇数、对初始质心的选择敏感、对异常值敏感等。</p><p>基于划分的聚类算法<br>K-Means 聚类算法<br>K-Means 聚类算法是一种非层次聚类算法，它将数据集分为K个簇，每个簇的中心点被称为质心。K-Means 算法的基本思想是通过不断迭代，将数据点划分到最近的质心所在的簇中，然后重新计算每个簇的质心，直到簇中心不再发生变化或达到最大迭代次数为止。<br>步骤：<br>随机的选取K个中心点，代表K个类别；<br>计算N个样本点和K个中心点之间的欧氏距离；<br>将每个样本点划分到最近的（欧氏距离最小的）中心点类别中——迭代1；<br>计算每个类别中样本点的均值，得到K个均值，将K个均值作为新的中心点——迭代2；<br>重复步骤2、3、4；<br>满足收敛条件后，得到收敛后的K个中心点（中心点不再变化）。<br>python代码实现：<br>#生成随机点<br>import matplotlib.pyplot as plt<br>import numpy as np<br>from sklearn.datasets.samples_generator import make_blobs<br>X, y_true &#x3D; make_blobs(n_samples&#x3D;300, centers&#x3D;4,<br>                       cluster_std&#x3D;0.60, random_state&#x3D;0)<br>plt.scatter(X[:, 0], X[:, 1], s&#x3D;50)<br>plt.show()</p><p>#K-Means 聚类算法<br>from sklearn.cluster import KMeans<br>“””<br>    KMeans(n_clusters&#x3D;8, init&#x3D;’k-means++’, n_init&#x3D;10, max_iter&#x3D;300,<br>            tol&#x3D;0.0001, precompute_distances&#x3D;’auto’, verbose&#x3D;0,<br>            random_state&#x3D;None, copy_x&#x3D;True, n_jobs&#x3D;1, algorithm&#x3D;’auto’)<br>        Parameters:<br>             n_clusters: 聚类个数<br>             max_iter：  最大迭代数<br>             n_init：    用不同的质心初始化值运行算法的次数<br>             init：      初始化质心的方法<br>             precompute_distances：预计算距离<br>             tol：       关于收敛的参数<br>             n_jobs：    计算的进程数<br>             random_state： 随机种子<br>             copy_x：是否修改原始数据<br>             algorithm：“auto”, “full” or “elkan”<br>                         ”full”就是我们传统的K-Means算法，<br>                         “elkan”elkan K-Means算法。默认的<br>                         ”auto”则会根据数据值是否是稀疏的，来决定如何选择”full”和“elkan”,稠密的选 “elkan”，否则就是”full”<br>        Attributes：<br>             cluster_centers_：质心坐标<br>             Labels_: 每个点的分类<br>             inertia_：每个点到其簇的质心的距离之和。<br>“””<br>m_kmeans &#x3D; KMeans(n_clusters&#x3D;4)<br>from sklearn import metrics</p><p>def draw(m_kmeans,X,y_pred,n_clusters):<br>    centers &#x3D; m_kmeans.cluster_centers_<br>    print(centers)<br>    plt.scatter(X[:, 0], X[:, 1], c&#x3D;y_pred, s&#x3D;50, cmap&#x3D;’viridis’)<br>    #中心点（质心）用红色标出<br>    plt.scatter(centers[:, 0], centers[:, 1], c&#x3D;’red’, s&#x3D;200, alpha&#x3D;0.5)<br>    print(“Calinski-Harabasz score：%lf”%metrics.calinski_harabasz_score(X, y_pred) )<br>    plt.title(“K-Means (clusters &#x3D; %d)”%n_clusters,fontsize&#x3D;20)<br>    plt.show()<br>m_kmeans.fit(X)<br>KMeans(algorithm&#x3D;’auto’, copy_x&#x3D;True, init&#x3D;’k-means++’, max_iter&#x3D;300,<br>    n_clusters&#x3D;4, n_init&#x3D;10, n_jobs&#x3D;None, precompute_distances&#x3D;’auto’,<br>    random_state&#x3D;None, tol&#x3D;0.0001, verbose&#x3D;0)<br>y_pred &#x3D; m_kmeans.predict(X)<br>draw(m_kmeans,X,y_pred,4)<br>K-Means 聚类算法升级</p><p>K-Means++ 算法：K-Means++ 算法是 K-Means 算法的改进版，它通过改进初始质心的选择方式，提高了算法的聚类效果。K-Means++ 算法的初始质心选择方式是在数据集中随机选择一个点作为第一个质心，然后选择与前面已选质心距离最大的点作为下一个质心，直到选出 K 个质心。与 K-Means 算法相比，K-Means++ 算法的聚类效果更好，收敛速度更快。<br>Mini-Batch K-Means 算法：Mini-Batch K-Means 算法是 K-Means 算法的一种变体，它采用了一种随机梯度下降的方式更新质心，从而加速了算法的收敛速度。Mini-Batch K-Means 算法将数据集划分为多个小批量，每个小批量包含一部分数据，然后在每个小批量上执行 K-Means 算法，更新质心。与 K-Means 算法相比，Mini-Batch K-Means 算法的计算速度更快，但聚类效果可能略有下降。<br>K-Means++-C 算法：K-Means++-C 算法是一种基于 K-Means++ 算法的并行聚类算法，它可以加速大规模数据集的聚类过程。K-Means++-C 算法将数据集划分为多个子集，然后在每个子集上执行 K-Means++ 算法，得到每个子集的聚类结果。最后，将所有子集的聚类结果合并，得到最终的聚类结果。与 K-Means++ 算法相比，K-Means++-C 算法的计算速度更快，适用于大规模数据集的聚类。 这些升级版的 K-Means 算法可以根据不同的需求选择使用，以获得更好的聚类效果和更快的计算速度。<br>K-Means++ 算法对比K-Means 聚类算法示例<br>#导入必要的库<br>import numpy as np<br>from sklearn.datasets import make_blobs<br>import matplotlib.pyplot as plt</p><p>#生成数据集<br>X, y &#x3D; make_blobs(n_samples&#x3D;300, centers&#x3D;4, cluster_std&#x3D;0.60, random_state&#x3D;0)</p><p>#定义 K-Means++ 聚类算法函数<br>def k_means_pp(X, K):<br>    centroids &#x3D; []<br>    # 随机选择一个初始质心<br>    i &#x3D; np.random.randint(0, len(X))<br>    centroids.append(X[i])<br>    # 选择剩余的质心<br>    for k in range(1, K):<br>        # 计算每个样本离最近质心的距离的平方和<br>        D2 &#x3D; np.array([min([np.linalg.norm(x-c)**2 for c in centroids]) for x in X])<br>        # 按照概率分布选择下一个质心<br>        probs &#x3D; D2 &#x2F; D2.sum()<br>        cumprobs &#x3D; probs.cumsum()<br>        r &#x3D; np.random.rand()<br>        j &#x3D; np.where(cumprobs &gt;&#x3D; r)[0][0]<br>        centroids.append(X[j])<br>    return centroids</p><p>#定义 K-Means 聚类算法函数<br>def k_means(X, K, centroids):<br>    # 初始化聚类结果、距离矩阵和聚类中心<br>    clusters &#x3D; np.zeros(len(X))<br>    D &#x3D; np.zeros((len(X), K))<br>    for k in range(K):<br>        D[:, k] &#x3D; np.linalg.norm(X - centroids[k], axis&#x3D;1)<br>    # 迭代更新聚类结果、距离矩阵和聚类中心<br>    while True:<br>        # 更新聚类结果<br>        new_clusters &#x3D; np.argmin(D, axis&#x3D;1)<br>        if np.array_equal(new_clusters, clusters):<br>            break<br>        clusters &#x3D; new_clusters<br>        # 更新聚类中心<br>        centroids &#x3D; [X[clusters &#x3D;&#x3D; k].mean(axis&#x3D;0) for k in range(K)]<br>        # 更新距离矩阵<br>        for k in range(K):<br>            D[:, k] &#x3D; np.linalg.norm(X - centroids[k], axis&#x3D;1)<br>    return clusters, centroids</p><p>#调用函数进行聚类<br>K &#x3D; 4<br>centroids &#x3D; k_means_pp(X, K)<br>clusters, centroids &#x3D; k_means(X, K, centroids)</p><p>#可视化聚类结果<br>plt.scatter(X[:, 0], X[:, 1], c&#x3D;clusters)<br>plt.scatter(np.array(centroids)[:, 0], np.array(centroids)[:, 1], marker&#x3D;’*’, s&#x3D;200, c&#x3D;’k’)<br>plt.show()</p><p>基于密度的聚类算法</p><p>DBSCAN聚类算法</p><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）聚类算法是一种基于密度的聚类算法，它可以自动识别任意形状的聚类簇，并能够有效地处理包含噪声的数据。DBSCAN 算法的基本思想是通过寻找数据点周围的相邻点，以及这些相邻点周围的相邻点，来判断数据点是否属于同一个簇中。<br>示例：<br>import numpy as np<br>from sklearn.cluster import DBSCAN<br>from sklearn.datasets import make_blobs<br>import matplotlib.pyplot as plt</p><h1 id="生成随机数据"><a href="#生成随机数据" class="headerlink" title="生成随机数据"></a>生成随机数据</h1><p>X, y &#x3D; make_blobs(n_samples&#x3D;1000, centers&#x3D;3, cluster_std&#x3D;0.5, random_state&#x3D;0)</p><h1 id="使用DBSCAN算法进行聚类"><a href="#使用DBSCAN算法进行聚类" class="headerlink" title="使用DBSCAN算法进行聚类"></a>使用DBSCAN算法进行聚类</h1><p>dbscan &#x3D; DBSCAN(eps&#x3D;0.5, min_samples&#x3D;5)<br>y_pred &#x3D; dbscan.fit_predict(X)</p><h1 id="绘制聚类结果"><a href="#绘制聚类结果" class="headerlink" title="绘制聚类结果"></a>绘制聚类结果</h1><p>plt.scatter(X[:, 0], X[:, 1], c&#x3D;y_pred)<br>plt.title(‘DBSCAN Clustering’)<br>plt.show()</p><p>Mean Shift聚类算法<br>Mean Shift 算法是一种基于密度的非参数聚类算法。它通过寻找密度函数的局部最大值来确定聚类簇的中心，进而将数据点分配到相应的聚类簇中。具体来说，Mean Shift 算法从任意一个数据点开始，通过迭代的方式不断移动该点，直到达到局部最大值为止。在移动过程中，Mean Shift 算法会不断地将数据点向密度函数值最大的方向移动，直到无法再移动为止，这样就能找到该点所在的聚类簇中心。然后，对于每个中心，可以找到与之相似的数据点并将它们分配到该簇中。最终，所有数据点都被分配到相应的聚类簇中。</p><p>import numpy as np</p><p>def euclidean_distance(x1, x2):<br>    return np.sqrt(np.sum((x1 - x2)**2))<br>#Mean Shift聚类算法<br>class MeanShift:<br>    def <strong>init</strong>(self, radius&#x3D;4):<br>        self.radius &#x3D; radius</p><pre><code>def fit(self, X):    centroids = []    for i in range(len(X)):        point = X[i]        while True:            # Find all points within the given radius            neighbors = []            for j in range(len(X)):                if euclidean_distance(point, X[j]) &lt;= self.radius:                    neighbors.append(X[j])            # Calculate the mean of all points within the radius            new_point = np.mean(neighbors, axis=0)            # If the new point is within a certain distance of the last point, stop            if euclidean_distance(point, new_point) &lt; 0.00001:                break            point = new_point        centroids.append(point)    self.centroids = np.unique(centroids, axis=0)def predict(self, X):    labels = []    for i in range(len(X)):        distances = [euclidean_distance(X[i], c) for c in self.centroids]        label = np.argmin(distances)        labels.append(label)    return labels</code></pre><p>测试：<br>from sklearn.datasets import make_blobs<br>import matplotlib.pyplot as plt</p><p>X, y &#x3D; make_blobs(n_samples&#x3D;200, centers&#x3D;3, cluster_std&#x3D;0.5, random_state&#x3D;0)</p><p>model &#x3D; MeanShift()<br>model.fit(X)</p><p>labels &#x3D; model.predict(X)</p><p>plt.scatter(X[:, 0], X[:, 1], c&#x3D;labels)<br>plt.show()</p><p>层次聚类算法</p><p>凝聚层次聚类（Aglomerative Hierarchical Clustering，简称 AGNES）</p><p>从一个个单独的数据点开始，不断合并最近的两个聚类簇，直到所有数据点都被合并成一个聚类簇为止。凝聚层次聚类的核心是距离计算和聚类合并规则的选择。常用的距离计算方法有欧几里得距离、曼哈顿距离、余弦距离等。常用的聚类合并规则有最小距离法、最大距离法、重心法等。<br>分裂层次聚类（Divisive Hierarchical Clustering，简称 DIANA）</p><p>从所有数据点的整体开始，不断将数据点划分成两个或多个子集，直到每个子集都成为一个聚类簇为止。分裂层次聚类的核心是划分方法的选择，常用的划分方法有 K-Means、PAM 等。 层次聚类算法的优点在于可以自动确定聚类簇的数量和层次结构，但其缺点在于算法的时间复杂度较高，在大数据集上的计算时间可能非常长，并且容易受到噪声数据和异常值的影响。<br>基于图的聚类算法</p><p>谱聚类算法（Spectral Clustering）</p><p>将数据点看作图上的节点，通过计算节点之间的相似度矩阵和拉普拉斯矩阵来确定节点之间的相似性和距离，然后通过对拉普拉斯矩阵进行特征分解来得到聚类簇的划分。<br>import numpy as np<br>import scipy.spatial.distance as dist</p><p>class SpectralClustering:<br>    def <strong>init</strong>(self, n_clusters&#x3D;2, affinity&#x3D;’rbf’, gamma&#x3D;1.0):<br>        self.n_clusters &#x3D; n_clusters<br>        self.affinity &#x3D; affinity<br>        self.gamma &#x3D; gamma</p><pre><code>def fit_predict(self, X):    # 计算相似度矩阵    if self.affinity == &#39;rbf&#39;:        S = self._rbf_kernel(X, gamma=self.gamma)    else:        S = self._knn_kernel(X, k=self.gamma)    # 计算拉普拉斯矩阵    L = self._laplacian_matrix(S)    # 计算特征值和特征向量    eigvals, eigvecs = np.linalg.eig(L)    # 取出前k个特征向量    idx = eigvals.argsort()[:self.n_clusters]    U = eigvecs[:, idx]    # 对U进行归一化处理    norm = np.linalg.norm(U, axis=1, keepdims=True)    U_norm = U / norm    # 对U_norm进行k-means聚类    from sklearn.cluster import KMeans    kmeans = KMeans(n_clusters=self.n_clusters)    return kmeans.fit_predict(U_norm)def _rbf_kernel(self, X, gamma):    dists = dist.squareform(dist.pdist(X, &#39;euclidean&#39;))    return np.exp(-gamma * dists ** 2)def _knn_kernel(self, X, k):    dists = dist.squareform(dist.pdist(X, &#39;euclidean&#39;))    W = np.zeros_like(dists)    for i in range(X.shape[0]):        idx = np.argsort(dists[i])[:k+1]        W[i, idx] = 1    return Wdef _laplacian_matrix(self, S):    # 计算度矩阵    D = np.diag(np.sum(S, axis=1))    # 计算拉普拉斯矩阵    L = D - S    return L</code></pre><p>社区发现算法（Community Detection）</p><p>将数据点看作社交网络上的节点，通过计算节点之间的社交关系和相似度来确定节点之间的相似性，然后利用社区发现算法将相似度高的节点划分到同一个社区中。<br>import networkx as nx<br>import community</p><h1 id="构建图"><a href="#构建图" class="headerlink" title="构建图"></a>构建图</h1><p>G &#x3D; nx.karate_club_graph()</p><h1 id="使用Louvain算法进行社区发现"><a href="#使用Louvain算法进行社区发现" class="headerlink" title="使用Louvain算法进行社区发现"></a>使用Louvain算法进行社区发现</h1><p>partition &#x3D; community.modularity_max.greedy_modularity_communities(G)</p><h1 id="输出每个节点属于哪个社区"><a href="#输出每个节点属于哪个社区" class="headerlink" title="输出每个节点属于哪个社区"></a>输出每个节点属于哪个社区</h1><p>for i, com in enumerate(partition):<br>    print(“Community %d: %s” % (i, com))</p><p>聚类算法使用领域</p><p>类算法是一种常用的无监督学习算法，可以将相似的数据点分组成簇，通常用于以下领域：<br>数据挖掘：聚类算法可以用于数据挖掘中的分析和分类，例如用户行为分析、产品定位、市场细分等。<br>图像处理：聚类算法可以用于图像分割、目标检测、图像压缩等应用，通过将图像像素聚类成不同的簇，实现对图像的分割和压缩。<br>自然语言处理：聚类算法可以用于文本聚类和主题模型，例如新闻分类、情感分析、文本推荐等。<br>生物信息学：聚类算法可以用于基因表达数据的聚类和分类，例如基因功能预测、新药研发等。<br>物联网：聚类算法可以用于传感器网络中的数据聚合和分类，例如智能交通、智能医疗等。 聚类算法在各个领域都有广泛的应用，可以帮助解决大规模数据的分类和分析问题，提高数据处理的效率和准确率。<br>总结</p><p>聚类算法是一种无监督学习算法，可以将相似的数据点分组成簇，是数据挖掘和机器学习领域中的重要技术之一。常用的聚类算法包括上文的K-Means、层次聚类、DBSCAN等。聚类算法的优点在于可以自动发现数据的内在结构和规律，可以用于数据挖掘、图像处理、自然语言处理、生物信息学、物联网等领域。在笔者看来，聚类算法的缺点在于需要大量计算资源和时间，容易受到噪声数据和异常值的影响，聚类结果也需要经过人工分析和解释，需要专业性，熟练性人才进入市场才能发挥这款无监督学习算法的强大作用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;聚类算法是一种无监督学习的算法，用于将数据集中的数据分成不同的聚类或组。聚类算法是数据挖掘和机器学习领域中常见的技术之一，具有广泛的应用。 以下是聚类算法的一些知识点：&lt;br&gt;聚类算法的目的是将数据集划分为不同的组，使得组内的数据点相似度高，组间的相似度低。&lt;br&gt;聚类算法</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>boilng新书《世界第二，美国“第一”》 序言 美国手持挥向世界的“镰刀”</title>
    <link href="https://jinchunqing1.github.io/2023/03/09/boilng%E6%96%B0%E4%B9%A6%E3%80%8A%E4%B8%96%E7%95%8C%E7%AC%AC%E4%BA%8C%EF%BC%8C%E7%BE%8E%E5%9B%BD%E2%80%9C%E7%AC%AC%E4%B8%80%E2%80%9D%E3%80%8B%20%E5%BA%8F%E8%A8%80%20%E7%BE%8E%E5%9B%BD%E6%89%8B%E6%8C%81%E6%8C%A5%E5%90%91%E4%B8%96%E7%95%8C%E7%9A%84%E2%80%9C%E9%95%B0%E5%88%80%E2%80%9D/"/>
    <id>https://jinchunqing1.github.io/2023/03/09/boilng%E6%96%B0%E4%B9%A6%E3%80%8A%E4%B8%96%E7%95%8C%E7%AC%AC%E4%BA%8C%EF%BC%8C%E7%BE%8E%E5%9B%BD%E2%80%9C%E7%AC%AC%E4%B8%80%E2%80%9D%E3%80%8B%20%E5%BA%8F%E8%A8%80%20%E7%BE%8E%E5%9B%BD%E6%89%8B%E6%8C%81%E6%8C%A5%E5%90%91%E4%B8%96%E7%95%8C%E7%9A%84%E2%80%9C%E9%95%B0%E5%88%80%E2%80%9D/</id>
    <published>2023-03-09T11:06:20.404Z</published>
    <updated>2023-03-11T14:47:03.439Z</updated>
    
    <content type="html"><![CDATA[<p>2022年世界最大的吸睛事件非“俄乌战争”莫属，劳塞维茨所著作的《战争论》中：“战争无非是政治通过另一种手段的继续”。一句话来解读，不管是军事还是政治亦或经济，种种手段都是为国家或民族利益服务的，它们之间不存在谁从属于谁的关系。战争只是国家意志的一种军事表现，是一种手段。政治自身，也不过是实现国家意志的一种手段。他们之间是相辅相成的关系，互为补充，互相支持，统一协调。<br>美国老牌精英政治家们对此则更加直接阐述：“战争就是政治的延续”。<br>在笔者看来精英政治家是服务于金融寡头所属的资本家阶层的，对于美国这样一个金融帝国来说，政治又是经济的延续。上世纪70年代，美国政府为了维持低失业率和高福利，不断超发货币，终于引发了一轮长达十七年的大通胀。经济学上是这样描述美国所引领的这一场通胀的：“福利主义盛行与宏观调控失据，导致滞胀局面形成并不断趋于恶化。”为了解决这场危机，1979年，新上任的美联储主席保罗沃尔赫开启了美国历史上最激进的暴力加息，一度将联邦基金利率拉升到前所未闻的20%。<br>在强有力的加息政策下，两年后，大通胀时代结束了，美国经济开启了新一轮增长，沃尔克也因此被奉为美联储历史上最负盛名的领导者。然而，实际上，真正的战场却在美国之外，欧洲德国等国接连发生经济衰退，货币竞相贬值，英国股市崩盘，南美洲阿根廷、巴西等国相继陷入债务违约，被迫向国际货币基金组织求救，亚洲第四次中东战争爆发，阿拉伯国家惨败，从而被迫接受美国的石油美元体系。<br>归根结底，美国解决通胀的办法只有一个，为超发的美元找到新的资产标的。最直接的办法是让有足够体量的经济体陷入危机，献出本国的优质资产。在之前分享过的经济学著作《认知世界的经济学》中明确提出宏观经济学的赚钱方法是:”经济危机、暴涨暴跌”。而美国是唯一拥有收割优质资产”镰刀“的政经联合体，加息只是这把”镰刀“最锋锐出的刃。<br>2022年，美国的通胀水平再次直逼沃尔克时代，伴随着俄乌战场的一声炮响，美联储同时开启了新一轮的加息周期，收割财富的镰刀再一次向全球挥下。<br>但是作为核心风向标的通胀率和就业率却迟迟不见下降。因为此时的美国已经不再是几十年前的美国，如今的世界已经不再是几十年前的世界，美国能否再续世界第二，美国“第一”的神话，让我们拭目以待。<br>时间来到2022年9月28日，伦敦金融城彻夜难眠。从美国高盛信贷销售团队流出的一份会议纪要让数百多名英国养老基金经理坐立难安。根据高盛的计算，由于近期英国国债价格暴跌，手握大量国债的英国养老基金需要向投行追加高达5500亿英镑的保证金。这不是目前英国养老基金所能负担的起的，一旦负担不起那就意味着基金破产，数百万英国人将失去养老金。英国可能会走向抗议，对立，甚至是民众抗议政府的暴乱。<br>在极度惶恐中度过了一夜后，基金经理们终于得到了一个振奋人心的消息，英国央行出手了，英格兰银行宣布临时无限量购债，为国债市场兜底，有了这份底气，英国养老基金方面终于稳住了局面。<br>但是随即则是经典轮回式名场面。为了避免再次出现流动性危机，英国养老基金正在计划以高达30%的折扣出售私人信贷、房地产等优质资产来换取现金。黑石集团等另外几家美国金融巨头随机加入了抢购的行列，英国养老基金预计将因此损失500多亿英镑的资产，资产再次向美国资本富集。美国资本成为了手握海量美元、时刻准备趁火打劫的镰刀上的放血漕。<br>2023年2月25日巴菲特向股东发出了第45封公开信，他坚信重资产的工业对利率更加敏感，受美联储加息周期的影响也更大。进入2023年，为了减轻现金流压力，德国化工巨头巴斯夫宣布裁员2600人，瑞典电信设备制造商爱立信宣布裁员8500人，瑞士制药巨头诺华宣布裁员8000人。美国已经瞄准那些能让美国心动的欧洲的核心工业和金融资产。<br>这里有一个让人玩味的例子，就是阿根廷所谓的”大豆美元“。去年旱灾袭击了阿根廷一半的国土，全国1&#x2F;4的耕地处于严重干旱状态。任何国家带着这样的负面状态进入美联储加息周期，结果都注定只有一个，外汇储备耗尽，政府因无力偿还外债而面临破产，国家将丧失借款能力，并无法进口必须的能源和物资。随着情况的恶化，阿根廷农业部、生产发展部、经济部等三个部级长官接连辞职，且迟迟找不到合适的继任者。直到2022年7月29日，51岁的塞尔吉奥马萨出人意料的同时接过了三个部长的职位。9月，马萨推出了一个让美国人也难以拒绝的天才创意，他提出了一个叫做大豆美元的计划。按照当时的官方汇率，一美元可以兑换145比索。马萨规定，凡是通过出口大豆赚取的美元，可以按照1:200的汇率兑换比索。可想而知，这项政策会造成两个结果，一是豆农售卖大豆的积极性会变高，而是所有大豆出口商将优先选择用美元结算，计划推出一个月后，阿根廷就出口了全国产量1&#x2F;3的大豆，换来了76亿美元的外汇，更重要的是，这项计划受到了美国的赞赏，尽管阿根廷的大豆国际贸易量每年只有200多亿美元，无法与上万亿美元的石油市场相提并论。但是这种创新的政策模式可能会被众多国家效仿，为此，美国有很强的动力将阿根廷的”大豆美元“树立成为一个标杆。<br>2023年才刚刚开始，美国似乎已经占领天时地利人和，但是未来会怎么样呢？美国能否延续世界第二，美国“第一”的经济神话。</p>]]></content>
    
    
    <summary type="html">经济学</summary>
    
    
    
    
    <category term="boiling" scheme="https://jinchunqing1.github.io/tags/boiling/"/>
    
  </entry>
  
</feed>
